{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3114e93f",
   "metadata": {},
   "source": [
    "https://github.com/Caldass/pl-matches-predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869889c",
   "metadata": {},
   "source": [
    "# PACKAGES IMPORTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2477dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283550c7",
   "metadata": {},
   "source": [
    "# FOLDER FOR DATA GATHERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32f1f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working directories\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'get_data','data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabc08d",
   "metadata": {},
   "source": [
    "# URLS FOR SEASONS SCRAPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710a55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of urls to scrape for all seasons\n",
    "#range of seasons wished to scrape\n",
    "seasons_wished = list(reversed(range(2008,2022)))\n",
    "\n",
    "#complement of all urls\n",
    "domain_name = 'https://www.oddsportal.com/'\n",
    "file_path = 'basketball/usa/nba'\n",
    "rest_of_url = '/results/'\n",
    "\n",
    "#first url = current season\n",
    "main_url = domain_name + file_path + rest_of_url\n",
    "\n",
    "#all other season url\n",
    "seasons_url = [domain_name + file_path + '-' + str(season) + '-' + str(season + 1) + rest_of_url for season in seasons_wished]\n",
    "\n",
    "#complete url list to be scraped\n",
    "all_url_seasons = [main_url] + seasons_url\n",
    "\n",
    "#all_user_seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d5e09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check if variable is null\n",
    "def is_empty(col):\n",
    "    try:\n",
    "        result = col.text\n",
    "    except:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3f72d",
   "metadata": {},
   "source": [
    "# OPTIONS DOE WEBDRIVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c831a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beautiful soup and selenium objects\n",
    "options = Options()\n",
    "\n",
    "#headless mode\n",
    "options.headless = True\n",
    "\n",
    "#disable useful tools\n",
    "options.add_argument(\"enable-automation\")\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "#maximized UI to load totally the page\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "#loading options for the driver \n",
    "driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba720466",
   "metadata": {},
   "source": [
    "# FUNCTION TO SCRAPE DATA FROM UNIQUE EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e28a526a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (622212451.py, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [17]\u001b[0;36m\u001b[0m\n\u001b[0;31m    ftd_hp' : ftd_hp,\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#scrape_data is a function which scrapes data event by event.\n",
    "#here are features scraped : \n",
    "    #ft_hp = full-time home points\n",
    "    #ft_ap = full-time away points\n",
    "    #ftd_hp = full-time home points with extra time thanks to draw. \n",
    "    #ftd_ap = full-time away points with extra time thanks to draw. \n",
    "    #aver_home_odd = average home winning odd\n",
    "    #aver_away_odd = average away winning odd\n",
    "    #high_home_odd = highest home winning odd\n",
    "    #high_away_odd = highest away winning odd\n",
    "    \n",
    "def scrape_data(url):\n",
    "    global ftd_hp, ftd_ap\n",
    "    #loading options for the driver \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #empty list for data scraped\n",
    "    data = {} \n",
    "    \n",
    "    #event-names\n",
    "    event_names = soup.find('div', attrs = {'id' : 'col-content'})\n",
    "    \n",
    "    #full-time result \n",
    "    ft_result = soup.find(attrs = {'class' : 'result'}) \n",
    "    \n",
    "    #extraction of average and highest data from soup\n",
    "    aver = soup.find('tr', attrs = {'class' : 'aver'})\n",
    "    high = soup.find('tr', attrs = {'class' : 'highest'})\n",
    "    \n",
    "    aver_home_odd, aver_away_odd = 0, 0\n",
    "    if aver:\n",
    "        #average home winning odd\n",
    "        aver_home_odd = aver.find('td', attrs = {'class' : \"right\"}).text\n",
    "        #average away winning odd\n",
    "        aver_away_odd =  aver.find('td', attrs = {'class' : \"right\"}).findNext( attrs = {'class' : \"right\"}).text\n",
    "\n",
    "    high_home_odd, high_away_odd = 0, 0\n",
    "    if high:    \n",
    "        #highest home winning odd\n",
    "        high_home_odd = high.find('td', attrs = {'class' : \"right\"}).text\n",
    "        #highest away winning odd\n",
    "        high_away_odd = high.find('td', attrs = {'class' : \"right\"}).findNext( attrs = {'class' : \"right\"}).text\n",
    "\n",
    "    ft_hp, ft_ap, ftd_hp, ftd_ap = 0, 0, 0, 0\n",
    "    #if ft_result:\n",
    "    if ft_result:\n",
    "        ft_hp = re.findall('[0-9]+', ft_result.find('strong').text)[0]\n",
    "        ft_ap = re.findall('[0-9]+', ft_result.find('strong').text)[1]\n",
    "        if len(re.findall('[0-9]+', ft_result.find('strong').text)) > 2:\n",
    "            ftd_hp = re.findall('[0-9]+', ft_result.find('strong').text)[2]\n",
    "            ftd_ap = re.findall('[0-9]+', ft_result.find('strong').text)[3]\n",
    "    \n",
    "    data = {\n",
    "    #event names    \n",
    "    'event_name' : event_names.find('h1').text,\n",
    "        \n",
    "    #timestamp\n",
    "    'timestamp' : event_names.find('p', attrs = {'class' : re.compile('date datet')}).text,\n",
    "        \n",
    "    #full-time home points\n",
    "    'ft_hp' : ft_hp,\n",
    "        \n",
    "    #full-time away points\n",
    "    'ft_ap' : ft_ap, \n",
    "        \n",
    "    #full-time with draw home points\n",
    "    ftd_hp' : ftd_hp,\n",
    "        \n",
    "    #full-time with draw away points\n",
    "    ftd_ap' : ftd_ap,\n",
    "        \n",
    "    #average home winning odd\n",
    "    aver_home_odd' : aver_home_odd,\n",
    "\n",
    "    #average away winning odd\n",
    "    aver_away_odd' : aver_away_odd,\n",
    "\n",
    "    #highest home winning odd\n",
    "    high_home_odd' : high_home_odd,\n",
    "        \n",
    "    #highest away winning odd\n",
    "    high_away_odd' : high_away_odd\n",
    "    }\n",
    "\n",
    "    driver.quit()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d36e40",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.42725133895874\n",
      "page 1 started\n"
     ]
    }
   ],
   "source": [
    "# Here we scrape only one season\n",
    "#url = all_url_seasons[3]\n",
    "url = 'https://www.oddsportal.com/basketball/usa/nba-2008-2009/results/#/page/26/'\n",
    "#loading options for the driver \n",
    "driver = webdriver.Chrome(options=options)\n",
    "t1load = time.time()\n",
    "\n",
    "#selenium and soup objects\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "t2load = time.time()\n",
    "print(t2load - t1load)\n",
    "\n",
    "event_data = []\n",
    "i = 0\n",
    "ii = 0\n",
    "\n",
    "t1loop = time.time()\n",
    "\n",
    "while True:\n",
    "    i = i + 1\n",
    "    print('page', i, 'started')\n",
    "    #get urls for each events\n",
    "    event_urls = []\n",
    "    for col in soup.find_all('tr', attrs = {'deactivate'}):\n",
    "        if col.find('td', attrs = {'center bold table-odds table-score'}).text != 'canc.':\n",
    "            event_urls.append(domain_name+col.find('a').attrs['href'])\n",
    "    #print(event_urls)\n",
    "    \n",
    "    #store previous or current page number\n",
    "    previous_page = soup.find_all( 'span', attrs = {'class' : 'active-page'})[0].text\n",
    "    \n",
    "    for event_url in event_urls:\n",
    "        ii = ii + 1\n",
    "        #print(ii)\n",
    "        event_data.append(scrape_data(event_url))\n",
    "    \n",
    "    #clicks on next page\n",
    "    element = driver.find_element(By.PARTIAL_LINK_TEXT, '»')\n",
    "    driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "    #sleep so that the page can load properly\n",
    "    time.sleep(2)\n",
    "\n",
    "    #reload soup objects on new page\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #get new page number\n",
    "    new_page = soup.find_all('span', attrs = {'class' : 'active-page'})[0].text\n",
    "\n",
    "    print('page', i, 'finished')\n",
    "    #if there's no new pages left break\n",
    "    if previous_page != new_page:\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "      \n",
    "    driver.quit()\n",
    "    \n",
    "t2loop = time.time()\n",
    "#print(t2loop - t1loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85284ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(event_data)\n",
    "df['country'] = 'usa'\n",
    "df['sport'] = 'basketball'\n",
    "df['league'] = 'nba'\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('usa_basketball_nba_'+url[46:55]+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
